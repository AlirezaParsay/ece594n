{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poincaré Embeddings for Learning Hierarchical Representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## I. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Set the context. What problem is this paper trying to solve? Why is this important?\n",
    "    Brief literature review: Identify 3+ related papers/models and briefly explain how they relate to one another (1 - 2 sentences per paper).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning algorithms first transform the feature space (*the sampled data of our dataset*) into a **\"latent space\"**  to **encode** pertitent information whilist often **reducing the dimensionality**, therefore reducing computational cost and memory usage, while **maintaining most of the important features** of the data.\n",
    "\n",
    "Ideally, in this latent space more similar inputs should be \"closer\" together when using a particular distance metric. More dissimilar inputs should be \"farther\" apart using the same distance metric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](assets/word2vecEmbedding.png) -->\n",
    "\n",
    "<img src=\"assets/word2vecEmbedding.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure depicts a particular embedding for English words. This particular implementation is known as **\"word2vec\"** and was ground-breaking at the time for being efficient to train and for encoding word similarity into a vectorspace. Two new prposed architectures generated the vector by either trying to predict the current word based on context (Bag-of-Words) or by predicting the surrounding words given the current word (Skip-gram). The vectors generated from these architectures allowed simple vector operations to be performed on words. Such as finding the vector from the embedding of \"man\" to \"woman\" and then applying this result to the embedding for \"king\". The result should be close to the embedding for \"queen\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](assets/glove.jpg) -->\n",
    "\n",
    "<img src=\"assets/glove.jpg\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While word2vec implicitly uses the co-occurance counts of words, another algorithm called **GloVe** uses a co-occurance matrix directly to create the word embeddings. This results in an embedding space with similar properities and similar performance. However, GloVe's method is more easily parallelizable which allows for larger corpora to be used which can increase its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](assets/node2vec.png) -->\n",
    "<img src=\"assets/node2vec.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are not just limited to words. Other algorithms have been developed to embed other forms of data into a vector space. One such example is **node2vec**. This algorithm can reduce the dimensionality of a graph by embedding nodes into a continuous vectorspace using biased random walks around the particular node's neighborhood. The resulting embeddings \"learn representations that organize nodes based on their network roles and/or communities they be-long to.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](assets/deepWalk.png) -->\n",
    "\n",
    "<img src=\"assets/deepWalk.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative to node2vec is **DEEPWALK**. It also embeds nodes in a graph in a linear continous latent space. Rather than a biased random walk, it just uses a pure random walk with no hyperparameters that can adjust the depth of the walk. After the walk, it uses the skip-gram model just like node2vec.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![title](assets/poincareEmbedding.png) -->\n",
    "\n",
    "<img src=\"assets/poincareEmbedding.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper proposes a new method of embedding data such that both similarity and hierarchy can be preserverd while also reducing the dimensionality. This is accomplished by using a non-linear latent space, an n-dimensional Poincaré ball. In some cases, hierarchy can be just as important as semantic similarity. One such example is taxonomical classifications of animals. Many different orders of mammal should all have about the same distance to the mammal point. This same embedding procedure can also be applied to data that orinially was not known to have a hierarchical structure. Another use case of this algorithm could be to confirm that existing hierarchical structures are based on real varying features rather than historical or cultural reasons.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Background & Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Introduce and explain the model and any necessary mathematical background. Use LaTeX for equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we intend to also\n",
    "reflect this hierarchy in the embedding space to improve over existing methods in two ways:\n",
    "1. By inducing an appropriate bias on the structure of the embedding space, we aim at learning\n",
    "more parsimonious embeddings for superior generalization performance and decreased\n",
    "runtime and memory complexity.\n",
    "2. By capturing the hierarchy explicitly in the embedding space, we aim at gaining additional\n",
    "insights about the relationships between symbols and the importance of individual symbols.\n",
    "\n",
    " we do not assume that we have direct access to information about the hierarchy, e.g.,\n",
    "via ordered input pairs. Instead, we consider the task of inferring the hierarchical relationships\n",
    "fully unsupervised, as is, for instance, necessary for text and network data. \n",
    "\n",
    "$g_x = (\\frac{2}{1-\\|{x}\\|^2}) g^E$\n",
    "\n",
    "$d(u,v) = arcosh(1+2\\frac{\\|u-v\\|^2}{(1-\\|u\\|^2)(1-\\|v\\|^2)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Implement the model (for the manifold used for the paper). All code should be documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Demonstration & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Demonstrate the model on a dataset and analyze the results, using visualizations when possible. The exact analyses you perform are up to you, but some general suggestions include:\n",
    "\n",
    "    Comparing the results to conventional algorithms already implemented in libraries such as scikit-learn. For example, if your model is Geodesic PCA, compare your results to conventional PCA run on the same dataset.\n",
    "    Demonstrating the effects of different hyperparameter choices\n",
    "    Analyzing computational cost\n",
    "    \n",
    "    Demonstrate the model on a dataset and analyze the results, using visualizations when possible. The exact analyses you perform are up to you, but some general suggestions include:\n",
    "\n",
    "    Comparing the results to conventional algorithms already implemented in libraries such as scikit-learn. For example, if your model is Geodesic PCA, compare your results to conventional PCA run on the same dataset.\n",
    "    Demonstrating the effects of different hyperparameter choices\n",
    "    Analyzing computational cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Examples: \n",
    "\n",
    "    You can find here learning algorithms already coded in Geomstats, that can help for inspiration.\n",
    "    You can find here notebooks that use some of these learning algorithms.\n",
    "    You can find here examples of implementations of geometric learning research paper, for an international challenge. Some were contributed by master students!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Nickel, Maximillian and Douwe Kiela (2017). “Poincaré Embeddings for Learning Hierarchical Representations”. In: Advances in Neural Information Processing Systems 30. Ed. by I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, et al. Curran Associates, Inc. -->\n",
    "\n",
    "Nickel, Maximillian and Douwe Kiela. Poincaré Embeddings for Learning Hierarchical Representations. Advances in Neural Information Processing Systems 30. Ed. by I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, et al. Curran Associates, Inc.\n",
    "\n",
    "<!-- Mikolov, Chen, Corrado and Dean (2013). \"Efficient Estimation of Word Representations in Vector Space\". In: arXiv:1301.3781 -->\n",
    "\n",
    "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\n",
    "representations of words and phrases and their compositionality. CoRR, abs/1310.4546, 2013.\n",
    "\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\n",
    "word representation. In EMNLP, volume 14, pages 1532–1543, 2014.\n",
    "\n",
    "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-\n",
    "sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\n",
    "discovery and data mining, pages 701–710. ACM, 2014.\n",
    "\n",
    "Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In\n",
    "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and\n",
    "Data Mining, pages 855–864. ACM, 2016."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
